# Klaudbiusz

AI-powered Databricks application generator with objective evaluation framework.

## Overview

Klaudbiusz generates production-ready Databricks applications from natural language prompts and evaluates them using 9 objective, zero-bias metrics. This enables autonomous deployment workflows where AI-generated code can be automatically validated and deployed without human review.

**Current Results:** 90% of generated apps (18/20) are production-ready and deployable.

## Quick Start

### Generate Applications

Klaudbiusz supports **two modes**:

**MCP Mode (default)** - Uses TypeScript/tRPC stack with MCP tools:
```bash
cd klaudbiusz
export DATABRICKS_HOST=https://your-workspace.databricks.com
export DATABRICKS_TOKEN=dapi...
export ANTHROPIC_API_KEY=sk-ant-...

# Generate a single app
uv run cli/main.py "Create a customer churn analysis dashboard"

# Batch generate from prompts
uv run cli/bulk_run.py
```

**Vanilla SDK Mode** - Pure Claude SDK with embedded context (Streamlit apps):
```bash
# Generate apps using pure Claude SDK (no MCP tools)
uv run cli/bulk_run.py --enable_mcp=False

# Single app in vanilla mode
uv run cli/main.py "Create dashboard" --enable_mcp=False
```

**Cost Comparison:**
- MCP Mode: $0.74/app, ~115 turns
- Vanilla SDK: **$0.27/app, ~33 turns** (63% cheaper, 71% fewer turns)

### Evaluate Generated Apps

**LLM-Based Stack-Agnostic Evaluation** - Works with any framework:

```bash
cd cli
export DATABRICKS_HOST=https://your-workspace.databricks.com
export DATABRICKS_TOKEN=dapi...
export ANTHROPIC_API_KEY=sk-ant-...  # Required for LLM-based discovery

# Evaluate all apps (auto-detects TypeScript, Python, Streamlit, etc.)
python3 evaluate_all.py

# Evaluate single app
python3 evaluate_app.py ../app/customer-churn-analysis
```

The evaluation uses an LLM to intelligently discover how to build, run, and test each app - no hardcoded assumptions about stack or structure.

## Evaluation Framework

We use **9 objective metrics** to measure autonomous deployability:

| Category | Metrics | Current Results |
|----------|---------|----------------|
| **Core Functionality** | Build, Runtime, Type Safety, Tests | 90%, 90%, 0%, 0% |
| **Databricks Integration** | DB Connectivity, Data Returned | 90%, 0% |
| **UI** | UI Renders | 0% |
| **Agentic DevX** | Local Runability, Deployability | 3.0/5, 3.0/5 |

**See [eval-docs/evals.md](eval-docs/evals.md) for complete metric definitions.**

### Key Innovation: Agentic DevX

We measure **whether an AI agent can autonomously run and deploy the code** with zero configuration:

- **Local Runability:** Can run with `npm install && npm start`? (3.0/5)
- **Deployability:** Can deploy with `docker build && docker run`? (3.0/5)

**See [eval-docs/DORA_METRICS.md](eval-docs/DORA_METRICS.md) for detailed agentic evaluation approach.**

## Documentation

### Framework & Methodology
- **[eval-docs/evals.md](eval-docs/evals.md)** - Complete 9-metric framework definition
- **[eval-docs/EVALUATION_METHODOLOGY.md](eval-docs/EVALUATION_METHODOLOGY.md)** - Zero-bias evaluation methodology
- **[eval-docs/DORA_METRICS.md](eval-docs/DORA_METRICS.md)** - DORA metrics integration & agentic DevX
- **[eval-docs/LLM_BASED_EVALUATION.md](eval-docs/LLM_BASED_EVALUATION.md)** - LLM-based stack-agnostic evaluation

### Generation Modes
- **[VANILLA_SDK_MODE.md](VANILLA_SDK_MODE.md)** - Pure Claude SDK mode (63% cheaper, no MCP tools)

### Results (Generated by Evaluation)
- **EVALUATION_REPORT.md** - Latest evaluation results (root level)
- **evaluation_report.json** - Structured data (root level)
- **evaluation_report.csv** - Spreadsheet format (root level)

### Archives
- **klaudbiusz_evaluation_*.tar.gz** - Historical evaluation archives
- **App.build Evals 2.0.docx** - Executive summary

## Project Structure

```
klaudbiusz/
├── README.md                        # This file
├── eval-docs/                       # Evaluation framework docs
│   ├── evals.md                    # 9-metric definitions
│   ├── EVALUATION_METHODOLOGY.md   # Zero-bias methodology
│   └── DORA_METRICS.md             # DORA & agentic DevX
├── app/                             # Generated applications (gitignored)
├── cli/                             # Generation & evaluation scripts
│   ├── bulk_run.py                 # Batch app generation
│   ├── evaluate_all.py             # Batch evaluation
│   ├── evaluate_app.py             # Single app evaluation
│   ├── archive_evaluation.sh       # Create evaluation archive
│   └── cleanup_evaluation.sh       # Clean generated apps
├── EVALUATION_REPORT.md            # Latest results (gitignored)
├── evaluation_report.json          # Latest data (gitignored)
├── evaluation_report.csv           # Latest spreadsheet (gitignored)
└── klaudbiusz_evaluation_*.tar.gz  # Archives
```

## Workflows

### Development Workflow

1. Write natural language prompt
2. Generate: `uv run cli/bulk_run.py`
3. Evaluate: `python3 cli/evaluate_all.py`
4. Review: `cat EVALUATION_REPORT.md`
5. Deploy apps that pass checks

### Archive & Clean Workflow

```bash
# Create archive of apps + reports
./cli/archive_evaluation.sh

# Verify checksum
shasum -a 256 -c klaudbiusz_evaluation_*.tar.gz.sha256

# Clean up generated apps
./cli/cleanup_evaluation.sh
```

## Requirements

- Python 3.11+
- uv (Python package manager)
- Docker (for builds and runtime checks)
- Node.js 18+ (for generated apps)
- Databricks workspace with access token

## Environment Variables

```bash
# Required for generation
export DATABRICKS_HOST=https://your-workspace.databricks.com
export DATABRICKS_TOKEN=dapi...
export ANTHROPIC_API_KEY=sk-ant-...

# Optional for logging
export DATABASE_URL=postgresql://...
```

## Core Principle

> If an AI agent cannot autonomously deploy its own generated code, that code is not production-ready.

All metrics are **objective, reproducible, and automatable** - no subjective quality assessments.

**See [eval-docs/EVALUATION_METHODOLOGY.md](eval-docs/EVALUATION_METHODOLOGY.md) for our zero-bias philosophy.**

## License

Apache 2.0

---

**Latest Evaluation:** October 17, 2025
**Success Rate:** 90% deployment-ready (18/20 apps)
**Lead Time:** 6-9 minutes (prompt → production-ready code)
