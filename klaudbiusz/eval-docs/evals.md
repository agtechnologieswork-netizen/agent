# Evaluation Metrics: Quick Reference

**Implementation details for the 9-metric framework. For design philosophy, see [EVALUATION_METHODOLOGY.md](EVALUATION_METHODOLOGY.md).**

**Stack Agnostic**: Evaluation uses LLM-based command discovery to work with any framework (TypeScript, Python, Streamlit, Flask, etc.) without hardcoded assumptions.

---

## The 9 Metrics

### 1. BUILD SUCCESS (Binary)
LLM-discovered build command exits with code 0 (e.g., `docker build`, `npm run build`, or no build if not needed)

### 2. RUNTIME SUCCESS (Binary)
Container starts using LLM-discovered run command, health check responds within 30s

### 3. TYPE SAFETY (Binary)
LLM-discovered type checking command passes with zero errors (e.g., `npx tsc --noEmit` for TypeScript, or skipped for Python)

### 4. TESTS PASS (Binary + Coverage %)
LLM-discovered test command succeeds, coverage reported if available (e.g., `npm test`, `pytest`, or skipped if no tests)

### 5. DATABRICKS CONNECTIVITY (Binary)
App connects to Databricks, executes queries without errors

### 6. DATA RETURNED (Binary)
**Status:** Not implemented (requires app-specific endpoint knowledge)

### 7. UI RENDERS (Binary) âœ…
**VLM check:** Screenshot shows content, no errors, page not blank
- Requires: Screenshot + `ANTHROPIC_API_KEY`
- Cost: ~$0.001 per check

### 8. LOCAL RUNABILITY (Score 0-5)
Checklist: README, .env.example, discovered install command works, discovered run command works, local startup

### 9. DEPLOYABILITY (Score 0-5)
Checklist: Dockerfile, multi-stage, health check, no secrets, deploy config

---

## AI Generation Metrics

Automatically tracked during `bulk_run.py`:
- **Cost (USD)**: Total API cost per app
- **Output Tokens**: Tokens generated by AI
- **Turns**: Number of conversation turns
- **Tokens/Turn**: Efficiency metric

---

## Output Formats

### JSON Structure
```json
{
  "summary": {
    "total_apps": 20,
    "metrics_summary": { /* 9 metrics aggregated */ },
    "generation_metrics": { /* cost, tokens, turns */ },
    "quality_distribution": { /* excellent/good/fair/poor */ }
  },
  "apps": [{
    "app_name": "...",
    "metrics": { /* 9 metrics */ },
    "generation_metrics": { /* cost, tokens, turns */ },
    "issues": []
  }]
}
```

### Reports Generated
- `evaluation_report.json` - Full structured data
- `evaluation_report.csv` - Flat table (9 metrics only)
- `EVALUATION_REPORT.md` - Human-readable markdown
- `evaluation_viewer.html` - Interactive web viewer

---

## Usage

```bash
# Required for LLM-based command discovery
export ANTHROPIC_API_KEY=sk-ant-...

# Generate apps (MCP mode - TypeScript/tRPC)
uv run cli/bulk_run.py

# Generate apps (Vanilla SDK mode - Streamlit/Python)
uv run cli/bulk_run.py --enable_mcp=False

# Evaluate all apps (stack-agnostic)
python3 cli/evaluate_all.py

# View results
open app-eval/evaluation_viewer.html
```

---

## Cost & Time

**Evaluation per app:**
- Time: 15-20 min (Docker build, tests, discovered install commands)
- Cost: ~$0.007 total
  - LLM command discovery: ~$0.006
  - VLM UI check: ~$0.001 (if screenshot exists)

**Generation per app (tracked):**
- MCP mode: $0.74, ~115 turns
- Vanilla SDK mode: $0.27, ~33 turns

---

## Troubleshooting

**LLM command discovery fails:**
- Ensure `ANTHROPIC_API_KEY` is set
- LLM analyzes app structure to discover build/test/run commands
- Falls back to empty commands if unavailable

**VLM check fails:**
- Ensure `ANTHROPIC_API_KEY` is set
- Screenshot must exist at `app_dir/screenshot_output/screenshot.png`

**Generation metrics missing:**
- Check `bulk_run_results_*.json` exists
- Verify `PROMPTS` dict in `bulk_run.py`

---

For detailed implementation, see:
- `cli/evaluate_all.py` - Batch evaluation
- `cli/evaluate_app.py` - Single app evaluation
- `cli/bulk_run.py` - App generation with metrics tracking
