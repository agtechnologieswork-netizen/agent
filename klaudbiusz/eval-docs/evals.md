# Evaluation Metrics: Quick Reference

**Implementation details for the 9-metric framework. For design philosophy, see [EVALUATION_METHODOLOGY.md](EVALUATION_METHODOLOGY.md).**

**Agentic Evaluation**: An AI agent with bash tools evaluates apps by reading files, executing commands, and measuring objective metrics. No hardcoded logic - the agent discovers how to build, run, and test each app regardless of framework.

---

## The 9 Metrics

### 1. BUILD SUCCESS (Binary)
Agent discovers build command from app files (package.json, Dockerfile, etc.), executes it, checks exit code 0

### 2. RUNTIME SUCCESS (Binary)
Agent discovers run command, starts app, verifies it runs without immediate errors

### 3. TYPE SAFETY (Binary)
Agent checks if app has type checking (tsc, mypy), runs it if present, reports pass/fail or N/A

### 4. TESTS PASS (Binary + Coverage %)
Agent checks if app has tests (npm test, pytest), runs them if present, reports pass/fail or N/A

### 5. DATABRICKS CONNECTIVITY (Binary)
Agent verifies app uses Databricks SQL connector and environment variables correctly

### 6. DATA RETURNED (Binary)
**Status:** Not implemented

### 7. UI RENDERS (Binary)
**Status:** Not implemented (requires screenshot)

### 8. LOCAL RUNABILITY (Score 0-5)
Checklist: README (1), .env.example (1), install works (1), run works (1), starts successfully (1)

### 9. DEPLOYABILITY (Score 0-5)
Checklist: Dockerfile (1), multi-stage build (1), health check (1), no hardcoded secrets (1), app.yaml (1)

---

## AI Generation Metrics

Automatically tracked during `bulk_run.py`:
- **Cost (USD)**: Total API cost per app
- **Output Tokens**: Tokens generated by AI
- **Turns**: Number of conversation turns
- **Tokens/Turn**: Efficiency metric

---

## Output Formats

### JSON Structure
```json
{
  "summary": {
    "total_apps": 20,
    "metrics_summary": { /* 9 metrics aggregated */ },
    "generation_metrics": { /* cost, tokens, turns */ },
    "quality_distribution": { /* excellent/good/fair/poor */ }
  },
  "apps": [{
    "app_name": "...",
    "metrics": { /* 9 metrics */ },
    "generation_metrics": { /* cost, tokens, turns */ },
    "issues": []
  }]
}
```

### Reports Generated
- `evaluation_report.json` - Full structured data
- `evaluation_report.csv` - Flat table (9 metrics only)
- `EVALUATION_REPORT.md` - Human-readable markdown
- `evaluation_viewer.html` - Interactive web viewer

---

## Usage

```bash
# Required for evaluation agent
export ANTHROPIC_API_KEY=sk-ant-...
export DATABRICKS_HOST=https://...
export DATABRICKS_TOKEN=dapi...

# Generate apps (MCP mode - TypeScript/tRPC)
uv run cli/bulk_run.py

# Generate apps (Vanilla SDK mode - Streamlit/Python)
uv run cli/bulk_run.py --enable_mcp=False

# Evaluate all apps (agentic evaluation)
uv run cli/evaluate_all_agent.py

# View results
cat evaluation_report.json
cat EVALUATION_REPORT.md
```

---

## Cost & Time

**Agentic Evaluation (all apps):**
- Time: Variable (agent discovers and executes commands for each app)
- Cost: ~$0.02-0.05 per full run (20 apps)
- Agent reads files, executes builds/tests, generates report

**Generation per app (tracked):**
- MCP mode: $0.74, ~115 turns
- Vanilla SDK mode: $0.27, ~33 turns

---

## Troubleshooting

**Evaluation agent fails:**
- Ensure `ANTHROPIC_API_KEY` is set
- Ensure `DATABRICKS_HOST` and `DATABRICKS_TOKEN` are set
- Check agent has permission to execute bash commands
- Review agent output for specific errors

**Apps not found:**
- Verify apps exist in ../app/ directory
- Check that bulk_run.py completed successfully

**Generation metrics missing:**
- Check `bulk_run_results_*.json` exists in app/ directory
- Verify `PROMPTS` dict in `bulk_run.py`

---

For detailed implementation, see:
- `cli/evaluate_all_agent.py` - Agentic evaluation script (~150 lines)
- `cli/bulk_run.py` - App generation with metrics tracking
- `eval-docs/EVALUATION_METHODOLOGY.md` - Zero-bias methodology
