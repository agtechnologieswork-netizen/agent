# Evaluation Metrics: Quick Reference

**Implementation details for the 9-metric framework. For design philosophy, see [EVALUATION_METHODOLOGY.md](EVALUATION_METHODOLOGY.md).**

---

## The 9 Metrics

### 1. BUILD SUCCESS (Binary)
`docker build` exits with code 0

### 2. RUNTIME SUCCESS (Binary)
Container starts, health check responds within 30s

### 3. TYPE SAFETY (Binary)
`npx tsc --noEmit` passes with zero errors

### 4. TESTS PASS (Binary + Coverage %)
All tests pass, coverage reported (target: ≥70%)

### 5. DATABRICKS CONNECTIVITY (Binary)
App connects to Databricks, executes queries without errors

### 6. DATA RETURNED (Binary)
**Status:** Not implemented (requires app-specific tRPC procedures)

### 7. UI RENDERS (Binary) ✅
**VLM check:** Screenshot shows content, no errors, page not blank
- Requires: Screenshot + `ANTHROPIC_API_KEY`
- Cost: ~$0.001 per check

### 8. LOCAL RUNABILITY (Score 0-5)
Checklist: README, .env.example, npm install, npm start, local startup

### 9. DEPLOYABILITY (Score 0-5)
Checklist: Dockerfile, multi-stage, health check, no secrets, deploy config

---

## AI Generation Metrics

Automatically tracked during `bulk_run.py`:
- **Cost (USD)**: Total API cost per app
- **Output Tokens**: Tokens generated by AI
- **Turns**: Number of conversation turns
- **Tokens/Turn**: Efficiency metric

---

## Output Formats

### JSON Structure
```json
{
  "summary": {
    "total_apps": 20,
    "metrics_summary": { /* 9 metrics aggregated */ },
    "generation_metrics": { /* cost, tokens, turns */ },
    "quality_distribution": { /* excellent/good/fair/poor */ }
  },
  "apps": [{
    "app_name": "...",
    "metrics": { /* 9 metrics */ },
    "generation_metrics": { /* cost, tokens, turns */ },
    "issues": []
  }]
}
```

### Reports Generated
- `evaluation_report.json` - Full structured data
- `evaluation_report.csv` - Flat table (9 metrics only)
- `EVALUATION_REPORT.md` - Human-readable markdown
- `evaluation_viewer.html` - Interactive web viewer

---

## Usage

```bash
# Generate apps
uv run python cli/bulk_run.py

# Evaluate all apps
uv run python cli/evaluate_all.py

# View results
open app-eval/evaluation_viewer.html
```

---

## Cost & Time

**Evaluation per app:**
- Time: 15-20 min (Docker build, tests, npm install)
- Cost: ~$0.001 (VLM check only, if screenshot exists)

**Generation per app (tracked):**
- Average: $0.74
- Turns: ~93
- Tokens/turn: ~173

---

## Troubleshooting

**Missing dependencies error:**
- Evaluation runs `npm install` automatically

**VLM check fails:**
- Ensure `ANTHROPIC_API_KEY` is set
- Screenshot must exist at `app_dir/screenshot_output/screenshot.png`

**Generation metrics missing:**
- Check `bulk_run_results_*.json` exists
- Verify `PROMPTS` dict in `bulk_run.py`

---

For detailed implementation, see:
- `cli/evaluate_all.py` - Batch evaluation
- `cli/evaluate_app.py` - Single app evaluation
- `cli/bulk_run.py` - App generation with metrics tracking
