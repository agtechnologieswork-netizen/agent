import re
from dataclasses import dataclass
from typing import List, Optional
import polars as pl
from databricks.sdk import WorkspaceClient
from databricks.sdk.service.sql import StatementState, State
from functools import lru_cache

from log import get_logger

logger = get_logger(__name__)


@dataclass
class TableMetadata:
    catalog: str
    schema: str
    name: str
    full_name: str
    table_type: str
    owner: Optional[str] = None
    comment: Optional[str] = None
    storage_location: Optional[str] = None
    data_source_format: Optional[str] = None
    created_at: Optional[str] = None
    updated_at: Optional[str] = None


@dataclass
class ColumnMetadata:
    name: str
    data_type: str
    comment: Optional[str] = None
    nullable: Optional[bool] = None
    position: Optional[int] = None


@dataclass
class TableDetails:
    metadata: TableMetadata
    columns: List[ColumnMetadata]
    sample_data: Optional[pl.DataFrame] = None
    row_count: Optional[int] = None
    size_bytes: Optional[int] = None


class DatabricksClient:

    def __init__(self, workspace_client: Optional[WorkspaceClient] = None):
        self.client = workspace_client or WorkspaceClient()
        logger.info("Initialized Databricks client")

    def _get_warehouse_id(self) -> str:
        """Get an available warehouse ID, preferring running warehouses."""
        running_warehouses = [x for x in self.client.warehouses.list() if x.state == State.RUNNING]
        if not running_warehouses:
            warehouses = list(self.client.warehouses.list())
            if not warehouses:
                raise RuntimeError("No warehouses available")
            warehouse = warehouses[0]
        else:
            warehouse = running_warehouses[0]
        if not warehouse.id:
            raise RuntimeError("Warehouse has no ID")
        return warehouse.id

    @lru_cache(maxsize=128)
    def list_tables(
        self,
        catalog: str = "samples",
        schema: str = "*",
        exclude_inaccessible: bool = True
    ) -> List[TableMetadata]:
        logger.info(f"Listing tables: catalog={catalog}, schema={schema}, exclude_inaccessible={exclude_inaccessible}")

        tables = []

        # Get list of catalogs
        if catalog == "*":
            catalogs = list(self.client.catalogs.list())
            catalog_names = [c.name for c in catalogs]
            logger.debug(f"Found {len(catalog_names)} catalogs")
        else:
            catalog_names = [catalog]

        # Iterate through catalogs
        for catalog_name in catalog_names:
            if not catalog_name:
                continue
            # Get list of schemas
            if schema == "*":
                schemas = list(self.client.schemas.list(catalog_name=catalog_name))
                schema_names = [s.name for s in schemas if s.name]
                logger.debug(f"Found {len(schema_names)} schemas in catalog {catalog_name}")
            else:
                schema_names = [schema]

            # Iterate through schemas
            for schema_name in schema_names:
                if not schema_name:
                    continue
                # List tables in schema
                table_list = list(self.client.tables.list(
                    catalog_name=catalog_name,
                    schema_name=schema_name
                ))
                logger.debug(f"Found {len(table_list)} tables in {catalog_name}.{schema_name}")

                for table in table_list:
                    # Skip if exclude_inaccessible is True and we can't access
                    if exclude_inaccessible and table.full_name and not self._has_table_access(table.full_name):
                        logger.debug(f"Skipping inaccessible table: {table.full_name}")
                        continue

                    # Skip tables with missing required fields
                    if not all([table.name, table.full_name, catalog_name, schema_name]):
                        logger.debug("Skipping table with missing required fields")
                        continue

                    tables.append(TableMetadata(
                        catalog=catalog_name,
                        schema=schema_name,
                        name=table.name,  # type: ignore
                        full_name=table.full_name,  # type: ignore
                        table_type=table.table_type.value if table.table_type else "UNKNOWN",
                        owner=table.owner,
                        comment=None,  # don't include comment in list_tables
                        storage_location=table.storage_location,
                        data_source_format=table.data_source_format.value if table.data_source_format else None,
                        created_at=str(table.created_at) if table.created_at else None,
                        updated_at=str(table.updated_at) if table.updated_at else None
                    ))

        logger.info(f"Found {len(tables)} accessible tables")
        return tables

    @lru_cache(maxsize=128)
    def get_table_details(self, table_full_name: str, sample_size: int = 10) -> TableDetails:
        logger.info(f"Getting details for table: {table_full_name}")

        # Parse table metadata
        parts = table_full_name.split(".")
        if len(parts) != 3:
            raise ValueError(f"Invalid table name format: {table_full_name}. Expected catalog.schema.table")

        # Get table metadata
        table = self.client.tables.get(table_full_name)

        metadata = TableMetadata(
            catalog=parts[0],
            schema=parts[1],
            name=parts[2],
            full_name=table_full_name,
            table_type=table.table_type.value if table.table_type else "UNKNOWN",
            owner=table.owner,
            comment=table.comment,
            storage_location=table.storage_location,
            data_source_format=table.data_source_format.value if table.data_source_format else None,
            created_at=str(table.created_at) if table.created_at else None,
            updated_at=str(table.updated_at) if table.updated_at else None
        )

        # Parse columns
        columns = []
        if table.columns:
            for i, col in enumerate(table.columns):
                if col.name and col.type_name:
                    columns.append(ColumnMetadata(
                        name=col.name,
                        data_type=str(col.type_name),
                        comment=col.comment,
                        nullable=col.nullable,
                        position=i
                    ))

        # Get sample data
        sample_data = None
        row_count = None

        # Execute sample query
        sample_query = f"SELECT * FROM {table_full_name} LIMIT {sample_size}"
        logger.debug(f"Executing sample query: {sample_query}")

        warehouse_id = self._get_warehouse_id()

        execution = self.client.statement_execution.execute_statement(
            warehouse_id=warehouse_id,
            statement=sample_query,
            wait_timeout="30s"
        )

        if execution.status and execution.status.state != StatementState.SUCCEEDED:
            raise RuntimeError(f"Sample query failed with state: {execution.status.state}")

        # Convert result to polars DataFrame
        if execution.result and execution.result.data_array and execution.manifest and execution.manifest.schema and execution.manifest.schema.columns:
            col_names = [col.name for col in execution.manifest.schema.columns if col.name]
            sample_data = pl.DataFrame(execution.result.data_array, schema=col_names, orient="row")
            logger.debug(f"Retrieved {len(sample_data)} sample rows")

        # Get row count
        count_query = f"SELECT COUNT(*) as count FROM {table_full_name}"
        execution = self.client.statement_execution.execute_statement(
            warehouse_id=warehouse_id,
            statement=count_query,
            wait_timeout="30s"
        )

        if execution.status and execution.status.state != StatementState.SUCCEEDED:
            raise RuntimeError(f"Count query failed with state: {execution.status.state}")

        if execution.result and execution.result.data_array:
            if execution.result.data_array:
                row_count = int(execution.result.data_array[0][0])
                logger.debug(f"Table has {row_count} rows")
            else:
                raise RuntimeError("Count query returned no results")
        else:
            raise RuntimeError("Count query returned no results")

        return TableDetails(
            metadata=metadata,
            columns=columns,
            sample_data=sample_data,
            row_count=row_count
        )

    def _has_table_access(self, table_full_name: str) -> bool:
        try:
            # Try to get table info - this will fail if no access
            self.client.tables.get(table_full_name)
            return True
        except Exception:
            return False

    def _is_read_only_query(self, query: str) -> bool:
        """Check if query is read-only by parsing for write operations."""
        # normalize query - remove comments and extra whitespace
        query_clean = re.sub(r'/\*.*?\*/', '', query, flags=re.DOTALL)  # remove /* */ comments
        query_clean = re.sub(r'--.*', '', query_clean)  # remove -- comments
        query_clean = re.sub(r'\s+', ' ', query_clean.strip()).upper()

        # check for write operations (more comprehensive than prefix matching)
        write_keywords = {
            'INSERT', 'UPDATE', 'DELETE', 'DROP', 'CREATE', 'ALTER',
            'TRUNCATE', 'REPLACE', 'MERGE', 'COPY', 'GRANT', 'REVOKE',
            'SET', 'USE', 'CALL'  # procedures/functions that might modify state
        }

        # split into tokens and check if any write keyword appears in statement context
        tokens = query_clean.split()
        if not tokens:
            return False

        # check main statement keywords (not in subqueries/CTEs)
        for token in tokens:
            if token in write_keywords:
                return False

        return True

    @lru_cache(maxsize=128)
    def execute_query(self, query: str, timeout: int = 45) -> pl.DataFrame:
        """Execute a SELECT query and return results as a polars DataFrame.

        Args:
            query: SQL query to execute (must be a SELECT statement)
            timeout: Query execution timeout in seconds (default: 45s)

        Returns:
            polars DataFrame with query results

        Raises:
            ValueError: If query is not a SELECT statement
            RuntimeError: If no warehouses available or query execution fails
        """

        timeout = min(timeout, 50)
        timeout_str = f"{timeout}s"

        # validate it's a read-only query for safety
        if not self._is_read_only_query(query):
            raise ValueError("Only SELECT queries are allowed")
        logger.info(f"Executing query: {query.replace('\n', ' ')}..., timeout {timeout}")

        # get available warehouse
        warehouse_id = self._get_warehouse_id()
        logger.debug(f"Using warehouse: {warehouse_id}")

        # execute the query
        execution = self.client.statement_execution.execute_statement(
            warehouse_id=warehouse_id,
            statement=query,
            wait_timeout=timeout_str
        )

        if execution.status and execution.status.state != StatementState.SUCCEEDED:
            error_msg = f"Query failed with state: {execution.status.state}"
            if execution.status.error:
                error_msg += f" - {execution.status.error.message}"
            raise RuntimeError(error_msg)

        # convert result to polars DataFrame
        if execution.manifest and execution.manifest.schema and execution.manifest.schema.columns:
            col_names = [col.name for col in execution.manifest.schema.columns if col.name]

            if execution.result and execution.result.data_array:
                df = pl.DataFrame(execution.result.data_array, schema=col_names, orient="row")
                logger.info(f"Query returned {len(df)} rows with {len(df.columns)} columns")
                return df
            else:
                # return empty DataFrame with schema preserved
                logger.info("Query returned no results, preserving schema")
                # create empty dataframe with correct schema
                schema = {col.name: pl.Utf8 for col in execution.manifest.schema.columns if col.name}
                return pl.DataFrame(schema=schema)
        else:
            # return empty DataFrame if no schema available
            logger.info("Query returned no results and no schema")
            return pl.DataFrame()


if __name__ == "__main__":
    # Example usage
    client = DatabricksClient()
    try:
        tables = client.list_tables()
        for table in tables:
            print(f"Table: {table.full_name}, Type: {table.table_type}")
            details = client.get_table_details(table.full_name)
            print(f"Columns: {[col.name for col in details.columns]}")
            if details.sample_data is not None:
                print(f"Sample data:\n{details.sample_data.head()}")
    except Exception as e:
        logger.error(f"Error: {e}")
